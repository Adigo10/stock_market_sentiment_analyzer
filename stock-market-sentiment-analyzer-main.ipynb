{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-04T08:13:10.018866Z",
     "iopub.status.busy": "2025-11-04T08:13:10.018266Z",
     "iopub.status.idle": "2025-11-04T08:14:29.249936Z",
     "shell.execute_reply": "2025-11-04T08:14:29.249173Z",
     "shell.execute_reply.started": "2025-11-04T08:13:10.018840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "!pip install nltk datasets transformers[torch] tokenizers evaluate rouge_score sentencepiece huggingface_hub scikit-learn matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:14:32.454201Z",
     "iopub.status.busy": "2025-11-04T08:14:32.453801Z",
     "iopub.status.idle": "2025-11-04T08:15:04.910772Z",
     "shell.execute_reply": "2025-11-04T08:15:04.909916Z",
     "shell.execute_reply.started": "2025-11-04T08:14:32.454162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 08:14:48.614670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762244088.810187      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762244088.866981      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: Tesla P100-PCIE-16GB\n",
      "GPU Memory: 17.06 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: \n",
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:15:33.193836Z",
     "iopub.status.busy": "2025-11-04T08:15:33.193570Z",
     "iopub.status.idle": "2025-11-04T08:15:33.281376Z",
     "shell.execute_reply": "2025-11-04T08:15:33.280686Z",
     "shell.execute_reply.started": "2025-11-04T08:15:33.193820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 4980\n",
      "\n",
      "Dataset columns: ['source_name', 'source', 'sentiment']\n",
      "\n",
      "First row:\n",
      "source_name                                              Reuters\n",
      "source         OpenAI announced Q1 2025 earnings that signifi...\n",
      "sentiment      <senti>Good<reason>Strong earnings beat and ra...\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment_label\n",
      "Good       2942\n",
      "Bad        1636\n",
      "Neutral     402\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample input-output pair:\n",
      "Input: Analyze the financial sentiment: OpenAI announced Q1 2025 earnings that significantly exceeded analyst expectations, driven by widespread adoption of ...\n",
      "Target: Sentiment: Good. Reason: Strong earnings beat and raised guidance typically drive positive investor sentiment and stock price appreciation....\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and Prepare Dataset \n",
    "# Load data\n",
    "df = pd.read_csv('/kaggle/input/stock-market-sentiment-analyzer/ai_stock_sentiment_5k.csv')\n",
    "\n",
    "print(f\"Total samples loaded: {len(df)}\")\n",
    "print(f\"\\nDataset columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst row:\")\n",
    "print(df.iloc[0])\n",
    "\n",
    "# Extract sentiment label and reason from format: <senti>Good<reason>explanation\n",
    "def extract_sentiment_label(sentiment_str):\n",
    "    \"\"\"Extract 'Good'/'Bad'/'Neutral' from '<senti>Good<reason>...' \"\"\"\n",
    "    try:\n",
    "        return sentiment_str.split('<reason>')[0].replace('<senti>', '').strip()\n",
    "    except:\n",
    "        return 'Neutral'\n",
    "\n",
    "def extract_reason(sentiment_str):\n",
    "    \"\"\"Extract reason from '<senti>Good<reason>...' \"\"\"\n",
    "    try:\n",
    "        return sentiment_str.split('<reason>')[1].strip()\n",
    "    except:\n",
    "        return \"No reason provided\"\n",
    "\n",
    "df['sentiment_label'] = df['sentiment'].apply(extract_sentiment_label)\n",
    "df['reason'] = df['sentiment'].apply(extract_reason)\n",
    "\n",
    "# Create target format: \"Sentiment: Good. Reason: ...\"\n",
    "df['target_text'] = df.apply(\n",
    "    lambda row: f\"Sentiment: {row['sentiment_label']}. Reason: {row['reason']}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create input format\n",
    "df['input_text'] = \"Analyze the financial sentiment: \" + df['source']\n",
    "\n",
    "# Check distribution\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "\n",
    "print(f\"\\nSample input-output pair:\")\n",
    "print(f\"Input: {df['input_text'].iloc[0][:150]}...\")\n",
    "print(f\"Target: {df['target_text'].iloc[0][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:15:36.204057Z",
     "iopub.status.busy": "2025-11-04T08:15:36.203779Z",
     "iopub.status.idle": "2025-11-04T08:15:36.242577Z",
     "shell.execute_reply": "2025-11-04T08:15:36.241920Z",
     "shell.execute_reply.started": "2025-11-04T08:15:36.204040Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "  Training: 3486 samples\n",
      "  Testing: 1494 samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Train/Test Split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create dataset dictionary\n",
    "dataset_dict = {\n",
    "    'input_text': df['input_text'].tolist(),\n",
    "    'target_text': df['target_text'].tolist(),\n",
    "    'sentiment_label': df['sentiment_label'].tolist()\n",
    "}\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split: 70% train, 30% test\n",
    "dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"  Training: {len(dataset['train'])} samples\")\n",
    "print(f\"  Testing: {len(dataset['test'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:15:38.047890Z",
     "iopub.status.busy": "2025-11-04T08:15:38.047327Z",
     "iopub.status.idle": "2025-11-04T08:15:43.878068Z",
     "shell.execute_reply": "2025-11-04T08:15:43.877398Z",
     "shell.execute_reply.started": "2025-11-04T08:15:38.047866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/flan-t5-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9763e65d20e405e94caf4aad5597004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f318a7086534a34a2cfe3422b09b70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002b7758cd224707bec32a632b0158cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9ef13cb9e247d1b3caf2756a07026e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9c5295104a4db5a7fb4bb336f1e8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7aa9bc42f04147b592adf413a98f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876fc918c48c4d1b9e9dd61934225041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Model parameters: 247,577,856\n",
      "Tokenizer vocab size: 32100\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Model and Tokenizer\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "print(f\"Model loaded successfully\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:15:46.914851Z",
     "iopub.status.busy": "2025-11-04T08:15:46.914556Z",
     "iopub.status.idle": "2025-11-04T08:15:49.341983Z",
     "shell.execute_reply": "2025-11-04T08:15:49.341194Z",
     "shell.execute_reply.started": "2025-11-04T08:15:46.914830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a93218e07e434b866cb7fd1312f530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd5ccc35e494e74b2f9f8f571908049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1494 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete\n",
      "Tokenized train samples: 3486\n",
      "Tokenized test samples: 1494\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Data Preprocessing and Tokenization\n",
    "# Prefix for the task\n",
    "prefix = \"Analyze the financial sentiment: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize inputs and labels\"\"\"\n",
    "    # The inputs are already prefixed in our dataset\n",
    "    inputs = examples[\"input_text\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    \n",
    "    # The labels are the target sentiment + reason\n",
    "    labels = tokenizer(text_target=examples[\"target_text\"], \n",
    "                       max_length=128,         \n",
    "                       truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "# Apply preprocessing to entire dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"Tokenization complete\")\n",
    "print(f\"Tokenized train samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Tokenized test samples: {len(tokenized_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:15:52.027432Z",
     "iopub.status.busy": "2025-11-04T08:15:52.026742Z",
     "iopub.status.idle": "2025-11-04T08:15:53.021198Z",
     "shell.execute_reply": "2025-11-04T08:15:53.020589Z",
     "shell.execute_reply.started": "2025-11-04T08:15:52.027411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3b73a2ade74d888c04b1646cfb485e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics configured\n",
      "Metrics: ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum, Sentiment Accuracy\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Setup Evaluation Metrics (ROUGE and Custom)\n",
    "# Download NLTK data\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute ROUGE scores and custom sentiment accuracy\"\"\"\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Extract sentiment accuracy\n",
    "    def extract_sentiment(text):\n",
    "        \"\"\"Extract sentiment from generated text\"\"\"\n",
    "        try:\n",
    "            if 'Sentiment:' in text:\n",
    "                sentiment = text.split('Sentiment:')[1].split('.')[0].strip()\n",
    "                if sentiment in ['Good', 'Bad', 'Neutral']:\n",
    "                    return sentiment\n",
    "            # Fallback\n",
    "            text_lower = text.lower()\n",
    "            if 'good' in text_lower or 'positive' in text_lower:\n",
    "                return 'Good'\n",
    "            elif 'bad' in text_lower or 'negative' in text_lower:\n",
    "                return 'Bad'\n",
    "            else:\n",
    "                return 'Neutral'\n",
    "        except:\n",
    "            return 'Neutral'\n",
    "    \n",
    "    pred_sentiments = [extract_sentiment(pred) for pred in decoded_preds]\n",
    "    true_sentiments = [extract_sentiment(label) for label in decoded_labels]\n",
    "    \n",
    "    sentiment_accuracy = accuracy_score(true_sentiments, pred_sentiments)\n",
    "    result['sentiment_accuracy'] = sentiment_accuracy\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Evaluation metrics configured\")\n",
    "print(\"Metrics: ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum, Sentiment Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:15:56.725175Z",
     "iopub.status.busy": "2025-11-04T08:15:56.724895Z",
     "iopub.status.idle": "2025-11-04T08:15:56.755986Z",
     "shell.execute_reply": "2025-11-04T08:15:56.755035Z",
     "shell.execute_reply.started": "2025-11-04T08:15:56.725155Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Epochs: 4\n",
      "  Batch size: 8\n",
      "  Learning rate: 0.0003\n",
      "  Checkpoints will be saved after each epoch\n",
      "  Total checkpoints to keep: 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: \n",
    "# Training hyperparameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 8  \n",
    "PER_DEVICE_EVAL_BATCH = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 2  \n",
    "NUM_EPOCHS = 4\n",
    "\n",
    "# Frequent checkpoints\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",  # Checkpoint after each epoch\n",
    "    learning_rate=L_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    save_total_limit=SAVE_TOTAL_LIM,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,  # Disable for stability\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"sentiment_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {L_RATE}\")\n",
    "print(f\"  Checkpoints will be saved after each epoch\")\n",
    "print(f\"  Total checkpoints to keep: {SAVE_TOTAL_LIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:15:59.268376Z",
     "iopub.status.busy": "2025-11-04T08:15:59.267750Z",
     "iopub.status.idle": "2025-11-04T08:16:04.009446Z",
     "shell.execute_reply": "2025-11-04T08:16:04.008672Z",
     "shell.execute_reply.started": "2025-11-04T08:15:59.268354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available storage:\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "/dev/loop1       20G   72K   20G   1% /kaggle/working\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8.5: Clean Up Storage\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Remove old results folder if exists\n",
    "if os.path.exists('./results'):\n",
    "    print(\"Removing old checkpoints...\")\n",
    "    shutil.rmtree('./results')\n",
    "    print(\"Old checkpoints removed\")\n",
    "\n",
    "# Check available space\n",
    "import subprocess\n",
    "result = subprocess.run(['df', '-h', '/kaggle/working'], capture_output=True, text=True)\n",
    "print(\"\\nAvailable storage:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:17:57.726774Z",
     "iopub.status.busy": "2025-11-04T08:17:57.726462Z",
     "iopub.status.idle": "2025-11-04T08:17:58.263662Z",
     "shell.execute_reply": "2025-11-04T08:17:58.262975Z",
     "shell.execute_reply.started": "2025-11-04T08:17:57.726755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully\n",
      "Training on 3486 samples\n",
      "Evaluating on 1494 samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")\n",
    "print(f\"Training on {len(tokenized_dataset['train'])} samples\")\n",
    "print(f\"Evaluating on {len(tokenized_dataset['test'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:18:01.512425Z",
     "iopub.status.busy": "2025-11-04T08:18:01.511709Z",
     "iopub.status.idle": "2025-11-04T08:34:54.942708Z",
     "shell.execute_reply": "2025-11-04T08:34:54.941766Z",
     "shell.execute_reply.started": "2025-11-04T08:18:01.512395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1744' max='1744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1744/1744 16:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Sentiment Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.418300</td>\n",
       "      <td>1.271406</td>\n",
       "      <td>0.468810</td>\n",
       "      <td>0.278829</td>\n",
       "      <td>0.435336</td>\n",
       "      <td>0.435613</td>\n",
       "      <td>0.952477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.201400</td>\n",
       "      <td>1.153397</td>\n",
       "      <td>0.483015</td>\n",
       "      <td>0.297761</td>\n",
       "      <td>0.452761</td>\n",
       "      <td>0.452864</td>\n",
       "      <td>0.951138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.991100</td>\n",
       "      <td>1.122992</td>\n",
       "      <td>0.494488</td>\n",
       "      <td>0.306597</td>\n",
       "      <td>0.463262</td>\n",
       "      <td>0.463394</td>\n",
       "      <td>0.951807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.805700</td>\n",
       "      <td>1.127825</td>\n",
       "      <td>0.494013</td>\n",
       "      <td>0.311405</td>\n",
       "      <td>0.464258</td>\n",
       "      <td>0.464223</td>\n",
       "      <td>0.959839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Training loss: 1.1665\n",
      "Training runtime: 1012.91 seconds\n",
      "Training samples per second: 13.77\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: \n",
    "print(\"=\"*60)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training runtime: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:35:01.295652Z",
     "iopub.status.busy": "2025-11-04T08:35:01.295217Z",
     "iopub.status.idle": "2025-11-04T08:35:03.083515Z",
     "shell.execute_reply": "2025-11-04T08:35:03.082679Z",
     "shell.execute_reply.started": "2025-11-04T08:35:01.295619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to: ./final_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Save Final Model\n",
    "final_model_path = \"./final_finetuned_model\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:35:07.920114Z",
     "iopub.status.busy": "2025-11-04T08:35:07.919419Z",
     "iopub.status.idle": "2025-11-04T08:37:47.328222Z",
     "shell.execute_reply": "2025-11-04T08:37:47.327403Z",
     "shell.execute_reply.started": "2025-11-04T08:35:07.920080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Evaluating on test set...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='374' max='374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [374/374 02:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  ROUGE-1: 0.4940\n",
      "  ROUGE-2: 0.3114\n",
      "  ROUGE-L: 0.4643\n",
      "  ROUGE-Lsum: 0.4642\n",
      "  Sentiment Accuracy: 0.9598\n",
      "  Evaluation Loss: 1.1278\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Evaluate on Test Set\n",
    "print(\"=\"*60)\n",
    "print(\"Evaluating on test set...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"  ROUGE-1: {eval_results['eval_rouge1']:.4f}\")\n",
    "print(f\"  ROUGE-2: {eval_results['eval_rouge2']:.4f}\")\n",
    "print(f\"  ROUGE-L: {eval_results['eval_rougeL']:.4f}\")\n",
    "print(f\"  ROUGE-Lsum: {eval_results['eval_rougeLsum']:.4f}\")\n",
    "print(f\"  Sentiment Accuracy: {eval_results['eval_sentiment_accuracy']:.4f}\")\n",
    "print(f\"  Evaluation Loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:39:04.821860Z",
     "iopub.status.busy": "2025-11-04T08:39:04.821577Z",
     "iopub.status.idle": "2025-11-04T08:55:13.659610Z",
     "shell.execute_reply": "2025-11-04T08:55:13.658809Z",
     "shell.execute_reply.started": "2025-11-04T08:39:04.821832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test set...\n",
      "  Processed 10/1494 samples...\n",
      "  Processed 20/1494 samples...\n",
      "  Processed 30/1494 samples...\n",
      "  Processed 40/1494 samples...\n",
      "  Processed 50/1494 samples...\n",
      "  Processed 60/1494 samples...\n",
      "  Processed 70/1494 samples...\n",
      "  Processed 80/1494 samples...\n",
      "  Processed 90/1494 samples...\n",
      "  Processed 100/1494 samples...\n",
      "  Processed 110/1494 samples...\n",
      "  Processed 120/1494 samples...\n",
      "  Processed 130/1494 samples...\n",
      "  Processed 140/1494 samples...\n",
      "  Processed 150/1494 samples...\n",
      "  Processed 160/1494 samples...\n",
      "  Processed 170/1494 samples...\n",
      "  Processed 180/1494 samples...\n",
      "  Processed 190/1494 samples...\n",
      "  Processed 200/1494 samples...\n",
      "  Processed 210/1494 samples...\n",
      "  Processed 220/1494 samples...\n",
      "  Processed 230/1494 samples...\n",
      "  Processed 240/1494 samples...\n",
      "  Processed 250/1494 samples...\n",
      "  Processed 260/1494 samples...\n",
      "  Processed 270/1494 samples...\n",
      "  Processed 280/1494 samples...\n",
      "  Processed 290/1494 samples...\n",
      "  Processed 300/1494 samples...\n",
      "  Processed 310/1494 samples...\n",
      "  Processed 320/1494 samples...\n",
      "  Processed 330/1494 samples...\n",
      "  Processed 340/1494 samples...\n",
      "  Processed 350/1494 samples...\n",
      "  Processed 360/1494 samples...\n",
      "  Processed 370/1494 samples...\n",
      "  Processed 380/1494 samples...\n",
      "  Processed 390/1494 samples...\n",
      "  Processed 400/1494 samples...\n",
      "  Processed 410/1494 samples...\n",
      "  Processed 420/1494 samples...\n",
      "  Processed 430/1494 samples...\n",
      "  Processed 440/1494 samples...\n",
      "  Processed 450/1494 samples...\n",
      "  Processed 460/1494 samples...\n",
      "  Processed 470/1494 samples...\n",
      "  Processed 480/1494 samples...\n",
      "  Processed 490/1494 samples...\n",
      "  Processed 500/1494 samples...\n",
      "  Processed 510/1494 samples...\n",
      "  Processed 520/1494 samples...\n",
      "  Processed 530/1494 samples...\n",
      "  Processed 540/1494 samples...\n",
      "  Processed 550/1494 samples...\n",
      "  Processed 560/1494 samples...\n",
      "  Processed 570/1494 samples...\n",
      "  Processed 580/1494 samples...\n",
      "  Processed 590/1494 samples...\n",
      "  Processed 600/1494 samples...\n",
      "  Processed 610/1494 samples...\n",
      "  Processed 620/1494 samples...\n",
      "  Processed 630/1494 samples...\n",
      "  Processed 640/1494 samples...\n",
      "  Processed 650/1494 samples...\n",
      "  Processed 660/1494 samples...\n",
      "  Processed 670/1494 samples...\n",
      "  Processed 680/1494 samples...\n",
      "  Processed 690/1494 samples...\n",
      "  Processed 700/1494 samples...\n",
      "  Processed 710/1494 samples...\n",
      "  Processed 720/1494 samples...\n",
      "  Processed 730/1494 samples...\n",
      "  Processed 740/1494 samples...\n",
      "  Processed 750/1494 samples...\n",
      "  Processed 760/1494 samples...\n",
      "  Processed 770/1494 samples...\n",
      "  Processed 780/1494 samples...\n",
      "  Processed 790/1494 samples...\n",
      "  Processed 800/1494 samples...\n",
      "  Processed 810/1494 samples...\n",
      "  Processed 820/1494 samples...\n",
      "  Processed 830/1494 samples...\n",
      "  Processed 840/1494 samples...\n",
      "  Processed 850/1494 samples...\n",
      "  Processed 860/1494 samples...\n",
      "  Processed 870/1494 samples...\n",
      "  Processed 880/1494 samples...\n",
      "  Processed 890/1494 samples...\n",
      "  Processed 900/1494 samples...\n",
      "  Processed 910/1494 samples...\n",
      "  Processed 920/1494 samples...\n",
      "  Processed 930/1494 samples...\n",
      "  Processed 940/1494 samples...\n",
      "  Processed 950/1494 samples...\n",
      "  Processed 960/1494 samples...\n",
      "  Processed 970/1494 samples...\n",
      "  Processed 980/1494 samples...\n",
      "  Processed 990/1494 samples...\n",
      "  Processed 1000/1494 samples...\n",
      "  Processed 1010/1494 samples...\n",
      "  Processed 1020/1494 samples...\n",
      "  Processed 1030/1494 samples...\n",
      "  Processed 1040/1494 samples...\n",
      "  Processed 1050/1494 samples...\n",
      "  Processed 1060/1494 samples...\n",
      "  Processed 1070/1494 samples...\n",
      "  Processed 1080/1494 samples...\n",
      "  Processed 1090/1494 samples...\n",
      "  Processed 1100/1494 samples...\n",
      "  Processed 1110/1494 samples...\n",
      "  Processed 1120/1494 samples...\n",
      "  Processed 1130/1494 samples...\n",
      "  Processed 1140/1494 samples...\n",
      "  Processed 1150/1494 samples...\n",
      "  Processed 1160/1494 samples...\n",
      "  Processed 1170/1494 samples...\n",
      "  Processed 1180/1494 samples...\n",
      "  Processed 1190/1494 samples...\n",
      "  Processed 1200/1494 samples...\n",
      "  Processed 1210/1494 samples...\n",
      "  Processed 1220/1494 samples...\n",
      "  Processed 1230/1494 samples...\n",
      "  Processed 1240/1494 samples...\n",
      "  Processed 1250/1494 samples...\n",
      "  Processed 1260/1494 samples...\n",
      "  Processed 1270/1494 samples...\n",
      "  Processed 1280/1494 samples...\n",
      "  Processed 1290/1494 samples...\n",
      "  Processed 1300/1494 samples...\n",
      "  Processed 1310/1494 samples...\n",
      "  Processed 1320/1494 samples...\n",
      "  Processed 1330/1494 samples...\n",
      "  Processed 1340/1494 samples...\n",
      "  Processed 1350/1494 samples...\n",
      "  Processed 1360/1494 samples...\n",
      "  Processed 1370/1494 samples...\n",
      "  Processed 1380/1494 samples...\n",
      "  Processed 1390/1494 samples...\n",
      "  Processed 1400/1494 samples...\n",
      "  Processed 1410/1494 samples...\n",
      "  Processed 1420/1494 samples...\n",
      "  Processed 1430/1494 samples...\n",
      "  Processed 1440/1494 samples...\n",
      "  Processed 1450/1494 samples...\n",
      "  Processed 1460/1494 samples...\n",
      "  Processed 1470/1494 samples...\n",
      "  Processed 1480/1494 samples...\n",
      "  Processed 1490/1494 samples...\n",
      "\n",
      "Prediction generation complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Generate Detailed Predictions\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "test_data = dataset['test']\n",
    "predictions = []\n",
    "true_labels = []\n",
    "generated_texts = []\n",
    "\n",
    "for i, example in enumerate(test_data):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(example['input_text'], return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract sentiments\n",
    "    def extract_sentiment(text):\n",
    "        try:\n",
    "            if 'Sentiment:' in text:\n",
    "                sentiment = text.split('Sentiment:')[1].split('.')[0].strip()\n",
    "                if sentiment in ['Good', 'Bad', 'Neutral']:\n",
    "                    return sentiment\n",
    "            text_lower = text.lower()\n",
    "            if 'good' in text_lower or 'positive' in text_lower:\n",
    "                return 'Good'\n",
    "            elif 'bad' in text_lower or 'negative' in text_lower:\n",
    "                return 'Bad'\n",
    "            else:\n",
    "                return 'Neutral'\n",
    "        except:\n",
    "            return 'Neutral'\n",
    "    \n",
    "    pred_sentiment = extract_sentiment(generated_text)\n",
    "    true_sentiment = example['sentiment_label']\n",
    "    \n",
    "    predictions.append(pred_sentiment)\n",
    "    true_labels.append(true_sentiment)\n",
    "    generated_texts.append(generated_text)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(test_data)} samples...\")\n",
    "\n",
    "print(\"\\nPrediction generation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:56:26.902223Z",
     "iopub.status.busy": "2025-11-04T08:56:26.901925Z",
     "iopub.status.idle": "2025-11-04T08:56:26.938969Z",
     "shell.execute_reply": "2025-11-04T08:56:26.938194Z",
     "shell.execute_reply.started": "2025-11-04T08:56:26.902203Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PERFORMANCE METRICS\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.9585 (95.85%)\n",
      "\n",
      "Per-Class Metrics:\n",
      "  Good:\n",
      "    Precision: 0.9730\n",
      "    Recall:    0.9942\n",
      "    F1-Score:  0.9835\n",
      "    Support:   869\n",
      "  Bad:\n",
      "    Precision: 0.9571\n",
      "    Recall:    0.9761\n",
      "    F1-Score:  0.9665\n",
      "    Support:   503\n",
      "  Neutral:\n",
      "    Precision: 0.8280\n",
      "    Recall:    0.6311\n",
      "    F1-Score:  0.7163\n",
      "    Support:   122\n",
      "\n",
      "Average Metrics:\n",
      "  Macro F1:    0.8888\n",
      "  Weighted F1: 0.9560\n",
      "\n",
      "Confusion Matrix:\n",
      "[[864   1   4]\n",
      " [  0 491  12]\n",
      " [ 24  21  77]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Calculate Performance Metrics\n",
    "print(\"=\"*60)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    true_labels, predictions, labels=['Good', 'Bad', 'Neutral'], average=None, zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\nPer-Class Metrics:\")\n",
    "for i, label in enumerate(['Good', 'Bad', 'Neutral']):\n",
    "    print(f\"  {label}:\")\n",
    "    print(f\"    Precision: {precision[i]:.4f}\")\n",
    "    print(f\"    Recall:    {recall[i]:.4f}\")\n",
    "    print(f\"    F1-Score:  {f1[i]:.4f}\")\n",
    "    print(f\"    Support:   {support[i]}\")\n",
    "\n",
    "# Macro and Weighted averages\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    true_labels, predictions, average='macro', zero_division=0\n",
    ")\n",
    "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "    true_labels, predictions, average='weighted', zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\nAverage Metrics:\")\n",
    "print(f\"  Macro F1:    {f1_macro:.4f}\")\n",
    "print(f\"  Weighted F1: {f1_weighted:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions, labels=['Good', 'Bad', 'Neutral'])\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:56:30.122061Z",
     "iopub.status.busy": "2025-11-04T08:56:30.121783Z",
     "iopub.status.idle": "2025-11-04T08:56:30.130207Z",
     "shell.execute_reply": "2025-11-04T08:56:30.129575Z",
     "shell.execute_reply.started": "2025-11-04T08:56:30.122041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAMPLE PREDICTIONS (First 5 from Test Set)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Sample 1\n",
      "============================================================\n",
      "Input: Analyze the financial sentiment: AMD launches new Instinct MI400 series with performance claims surpassing NVIDIA's Blackwell chips in specific benchmarks. The chips feature advanced memory architectu...\n",
      "\n",
      "True Output: Sentiment: Good. Reason: Competitive product launch and manufacturer support could help AMD gain market share in AI accelerators, positive for stock....\n",
      "\n",
      "Predicted Output: Sentiment: Good. Reason: Competitive product launch and major customer commitments should boost AMD's market position and stock valuation....\n",
      "\n",
      "True Sentiment: Good\n",
      "Predicted Sentiment: Good\n",
      "Match: ✓ Correct\n",
      "\n",
      "============================================================\n",
      "Sample 2\n",
      "============================================================\n",
      "Input: Analyze the financial sentiment: Intel announced disappointing Q1 2025 results and lowered full-year guidance, citing continued market share losses in data center chips to AMD and NVIDIA. The company'...\n",
      "\n",
      "True Output: Sentiment: Bad. Reason: Ongoing competitive pressures and guidance reduction will likely lead to further multiple compression and investor skepticism ...\n",
      "\n",
      "Predicted Output: Sentiment: Bad. Reason: Earnings miss and guidance reduction signal competitive struggles, likely leading to negative stock price reaction....\n",
      "\n",
      "True Sentiment: Bad\n",
      "Predicted Sentiment: Bad\n",
      "Match: ✓ Correct\n",
      "\n",
      "============================================================\n",
      "Sample 3\n",
      "============================================================\n",
      "Input: Analyze the financial sentiment: Tesla's Full Self-Driving (FSD) v13 software update was released to its entire North American fleet, marking the official removal of the beta label. Regulatory approva...\n",
      "\n",
      "True Output: Sentiment: Good. Reason: Regulatory milestone and commercialization of FSD represent a critical step towards realizing Tesla's high-margin software an...\n",
      "\n",
      "Predicted Output: Sentiment: Good. Reason: Removing a key product label and gaining regulatory approval is a major positive catalyst for the stock....\n",
      "\n",
      "True Sentiment: Good\n",
      "Predicted Sentiment: Good\n",
      "Match: ✓ Correct\n",
      "\n",
      "============================================================\n",
      "Sample 4\n",
      "============================================================\n",
      "Input: Analyze the financial sentiment: Meta launched its new open-source large language model, Llama 4, which it claims outperforms many closed models while being free for commercial and research use. The r...\n",
      "\n",
      "True Output: Sentiment: Good. Reason: Launching a competitive open-source model could increase Meta's influence in the AI ecosystem and attract developer talent, v...\n",
      "\n",
      "Predicted Output: Sentiment: Good. Reason: Launching a superior open-source model strengthens Meta's position in the AI ecosystem and could drive increased engagement a...\n",
      "\n",
      "True Sentiment: Good\n",
      "Predicted Sentiment: Good\n",
      "Match: ✓ Correct\n",
      "\n",
      "============================================================\n",
      "Sample 5\n",
      "============================================================\n",
      "Input: Analyze the financial sentiment: NVIDIA announced Q1 2025 earnings that shattered analyst expectations, with data center revenue surging 120% year-over-year due to unprecedented demand for its next-ge...\n",
      "\n",
      "True Output: Sentiment: Good. Reason: Massive earnings beat and strong forward guidance will likely drive significant stock price appreciation as AI demand continu...\n",
      "\n",
      "Predicted Output: Sentiment: Good. Reason: Massive earnings beat and strong forward guidance will likely drive significant stock price appreciation as AI demand narrati...\n",
      "\n",
      "True Sentiment: Good\n",
      "Predicted Sentiment: Good\n",
      "Match: ✓ Correct\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Show Sample Predictions\n",
    "print(\"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS (First 5 from Test Set)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(5, len(test_data))):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sample {i+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Input: {test_data[i]['input_text'][:200]}...\")\n",
    "    print(f\"\\nTrue Output: {test_data[i]['target_text'][:150]}...\")\n",
    "    print(f\"\\nPredicted Output: {generated_texts[i][:150]}...\")\n",
    "    print(f\"\\nTrue Sentiment: {true_labels[i]}\")\n",
    "    print(f\"Predicted Sentiment: {predictions[i]}\")\n",
    "    print(f\"Match: {'✓ Correct' if predictions[i] == true_labels[i] else '✗ Incorrect'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:56:36.616153Z",
     "iopub.status.busy": "2025-11-04T08:56:36.615845Z",
     "iopub.status.idle": "2025-11-04T08:56:36.851160Z",
     "shell.execute_reply": "2025-11-04T08:56:36.850473Z",
     "shell.execute_reply.started": "2025-11-04T08:56:36.616130Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed predictions saved to 'model_predictions.csv'\n",
      "Metrics summary saved to 'training_metrics_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Save Detailed Results\n",
    "# Save predictions CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'input': [ex['input_text'][:200] for ex in test_data],\n",
    "    'true_output': [ex['target_text'] for ex in test_data],\n",
    "    'predicted_output': generated_texts,\n",
    "    'true_sentiment': true_labels,\n",
    "    'predicted_sentiment': predictions,\n",
    "    'correct': [p == t for p, t in zip(predictions, true_labels)]\n",
    "})\n",
    "\n",
    "results_df.to_csv('model_predictions.csv', index=False)\n",
    "print(\"Detailed predictions saved to 'model_predictions.csv'\")\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_summary = {\n",
    "    'Model': 'flan-t5-base',\n",
    "    'Training Samples': len(tokenized_dataset['train']),\n",
    "    'Test Samples': len(tokenized_dataset['test']),\n",
    "    'Epochs': NUM_EPOCHS,\n",
    "    'Accuracy': accuracy,\n",
    "    'F1 (Macro)': f1_macro,\n",
    "    'F1 (Weighted)': f1_weighted,\n",
    "    'ROUGE-1': eval_results['eval_rouge1'],\n",
    "    'ROUGE-2': eval_results['eval_rouge2'],\n",
    "    'ROUGE-L': eval_results['eval_rougeL'],\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics_summary])\n",
    "metrics_df.to_csv('training_metrics_summary.csv', index=False)\n",
    "print(\"Metrics summary saved to 'training_metrics_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T08:56:58.017647Z",
     "iopub.status.busy": "2025-11-04T08:56:58.017378Z",
     "iopub.status.idle": "2025-11-04T09:02:22.936515Z",
     "shell.execute_reply": "2025-11-04T09:02:22.935809Z",
     "shell.execute_reply.started": "2025-11-04T08:56:58.017629Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating downloadable archives...\n",
      "  - all_checkpoints.zip (all training checkpoints)\n",
      "  - final_model.zip (final trained model)\n",
      "\n",
      "All archives created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Create Downloadable Archives\n",
    "import shutil\n",
    "\n",
    "print(\"Creating downloadable archives...\")\n",
    "\n",
    "# Archive all checkpoints\n",
    "shutil.make_archive('all_checkpoints', 'zip', './results')\n",
    "print(\"  - all_checkpoints.zip (all training checkpoints)\")\n",
    "\n",
    "# Archive final model\n",
    "shutil.make_archive('final_model', 'zip', './final_finetuned_model')\n",
    "print(\"  - final_model.zip (final trained model)\")\n",
    "\n",
    "print(\"\\nAll archives created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:02:38.404825Z",
     "iopub.status.busy": "2025-11-04T09:02:38.404554Z",
     "iopub.status.idle": "2025-11-04T09:02:41.235560Z",
     "shell.execute_reply": "2025-11-04T09:02:41.234816Z",
     "shell.execute_reply.started": "2025-11-04T09:02:38.404806Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL INFERENCE EXAMPLE\n",
      "============================================================\n",
      "\n",
      "Input Article:\n",
      "Apple Inc. announced record quarterly earnings with iPhone revenue up 25% year-over-year. \n",
      "The company also announced a new $90 billion stock buyback program and raised its dividend by 4%.\n",
      "CEO Tim Cook cited strong demand across all product categories.\n",
      "\n",
      "Model Prediction:\n",
      "Sentiment: Good. Reason: Strong earnings beat, massive buyback, and dividend increase are all highly bullish signals for investors.\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Model Inference Example\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL INFERENCE EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the final model\n",
    "inference_model = T5ForConditionalGeneration.from_pretrained(final_model_path)\n",
    "inference_tokenizer = T5Tokenizer.from_pretrained(final_model_path)\n",
    "\n",
    "# Example news article\n",
    "example_article = \"\"\"\n",
    "Apple Inc. announced record quarterly earnings with iPhone revenue up 25% year-over-year. \n",
    "The company also announced a new $90 billion stock buyback program and raised its dividend by 4%.\n",
    "CEO Tim Cook cited strong demand across all product categories.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input\n",
    "input_text = \"Analyze the financial sentiment: \" + example_article\n",
    "inputs = inference_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    outputs = inference_model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "\n",
    "prediction = inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nInput Article:\")\n",
    "print(example_article.strip())\n",
    "print(f\"\\nModel Prediction:\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T09:02:55.584052Z",
     "iopub.status.busy": "2025-11-04T09:02:55.583359Z",
     "iopub.status.idle": "2025-11-04T09:02:55.589945Z",
     "shell.execute_reply": "2025-11-04T09:02:55.589367Z",
     "shell.execute_reply.started": "2025-11-04T09:02:55.584031Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING AND EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy: 95.85%\n",
      "  F1-Score (Macro): 0.8888\n",
      "  F1-Score (Weighted): 0.9560\n",
      "  ROUGE-L: 0.4643\n",
      "\n",
      "Outputs Saved:\n",
      "  - Final model: ./final_finetuned_model/\n",
      "  - All checkpoints: ./results/\n",
      "  - Predictions: model_predictions.csv\n",
      "  - Metrics: training_metrics_summary.csv\n",
      "  - Confusion matrix: confusion_matrix.png\n",
      "  - Downloadable archives: all_checkpoints.zip, final_model.zip\n",
      "\n",
      "Checkpoints saved (one per epoch):\n",
      "  - checkpoint-1308\n",
      "  - checkpoint-1744\n"
     ]
    }
   ],
   "source": [
    "# Cell 20: Final Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING AND EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"  F1-Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"  F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "print(f\"  ROUGE-L: {eval_results['eval_rougeL']:.4f}\")\n",
    "\n",
    "print(f\"\\nOutputs Saved:\")\n",
    "print(f\"  - Final model: ./final_finetuned_model/\")\n",
    "print(f\"  - All checkpoints: ./results/\")\n",
    "print(f\"  - Predictions: model_predictions.csv\")\n",
    "print(f\"  - Metrics: training_metrics_summary.csv\")\n",
    "print(f\"  - Confusion matrix: confusion_matrix.png\")\n",
    "print(f\"  - Downloadable archives: all_checkpoints.zip, final_model.zip\")\n",
    "\n",
    "print(f\"\\nCheckpoints saved (one per epoch):\")\n",
    "import os\n",
    "checkpoints = [d for d in os.listdir('./results') if d.startswith('checkpoint-')]\n",
    "for cp in sorted(checkpoints):\n",
    "    print(f\"  - {cp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8631588,
     "sourceId": 13585947,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
